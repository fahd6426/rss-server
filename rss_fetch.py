import os
import requests
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import random

# =========================
# Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù…Ù† GitHub Secrets
# =========================
API_KEY = os.getenv("BLOGGER_API_KEY")
BLOG_ID = os.getenv("BLOG_ID")

# âœ… Ù…ØµØ§Ø¯Ø± RSS Ù…ÙØªÙˆØ­Ø©
RSS_FEEDS = [
    "https://www.kooora.com/rss/default.aspx",     # ÙƒÙˆÙˆÙˆØ±Ø©
    "https://www.espn.com/espn/rss/news",           # ESPN Ø£Ø®Ø¨Ø§Ø± Ø¹Ø§Ù…Ø©
]

# =========================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
# =========================
def load_posted_titles():
    try:
        with open("posted_titles.txt", "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    except FileNotFoundError:
        return set()

# =========================
# Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±
# =========================
def fetch_articles():
    all_articles = []
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± ...")

    for feed_url in RSS_FEEDS:
        print(f"ğŸ“¡ Ø¬Ù„Ø¨ Ù…Ù†: {feed_url}")
        try:
            resp = requests.get(feed_url, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ù…ØµØ¯Ø± {feed_url}: {e}")
            continue

        # Ù†Ø³ØªØ®Ø¯Ù… lxml-xml Ù„Ø£Ù†Ù†Ø§ Ø«Ø¨ØªÙ†Ø§Ù‡ ÙÙŠ Ø§Ù„Ù€ workflow
        soup = BeautifulSoup(resp.content, "lxml-xml")
        items = soup.find_all("item")
        print(f"â¡ï¸ ÙˆØ¬Ø¯Ù†Ø§ {len(items)} Ø®Ø¨Ø± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…ØµØ¯Ø±")

        for item in items:
            title = item.title.get_text(strip=True) if item.title else None
            description = item.description.get_text(strip=True) if item.description else ""
            # ØµÙˆØ±Ø© Ù„Ùˆ ÙÙŠÙ‡
            image_url = ""
            enclosure = item.find("enclosure")
            if enclosure and enclosure.get("url"):
                image_url = enclosure.get("url")

            if title:
                all_articles.append({
                    "title": title,
                    "content": description,
                    "image": image_url,
                    "category": "Ø±ÙŠØ§Ø¶Ø©"
                })

    print(f"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù„ÙŠ Ø¬Ù…Ø¹Ù†Ø§Ù‡Ø§ Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±: {len(all_articles)}")
    return all_articles

# =========================
# Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø¨Ø³ÙŠØ·Ø©
# =========================
def rephrase_content(content):
    intros = [
        "Ù†Ù‚Ø¯Ù… Ù„ÙƒÙ… ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ØªØ§Ù„ÙŠ: ",
        "ÙÙŠ Ù…ØªØ§Ø¨Ø¹Ø© Ù„Ø¢Ø®Ø± Ø§Ù„Ù…Ø³ØªØ¬Ø¯Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©: ",
        "Ø¥Ù„ÙŠÙƒÙ… Ø£Ø¨Ø±Ø² Ù…Ø§ ÙˆØ±Ø¯: "
    ]
    endings = [
        "ØªØ§Ø¨Ø¹ÙˆÙ†Ø§ Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.",
        "Ø²ÙˆØ±ÙˆØ§ Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© Ù„Ù„Ù…Ø²ÙŠØ¯.",
        "Ù†ÙˆØ§ÙÙŠÙƒÙ… Ø¨ÙƒÙ„ Ø¬Ø¯ÙŠØ¯."
    ]
    return f"{random.choice(intros)}{content} {random.choice(endings)}"

# =========================
# Ù†Ø´Ø± Ø¹Ù„Ù‰ Blogger
# =========================
def post_to_blogger(article, posted_titles):
    # Ù„Ùˆ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…ÙƒØ±Ø± Ù„Ø§ ØªÙ†Ø´Ø±
    if article["title"] in posted_titles:
        print(f'â­ ØªØ®Ø·Ù‘ÙŠ Ø®Ø¨Ø± Ù…ÙƒØ±Ø±: {article["title"]}')
        return

    try:
        service = build("blogger", "v3", developerKey=API_KEY)

        parts = []
        if article["image"]:
            parts.append(f'<img src="{article["image"]}" style="max-width:100%;">')
        parts.append(f'<h2>{article["title"]}</h2>')
        parts.append(f'<p>{rephrase_content(article["content"])}</p>')
        parts.append(f'<p>Ø§Ù„ØªØµÙ†ÙŠÙ: {article["category"]}</p>')
        parts.append('<p>Ø§Ù„Ù…ØµØ¯Ø±: Ù…ØµØ§Ø¯Ø± Ø±ÙŠØ§Ø¶ÙŠØ©</p>')

        content_html = "\n".join(parts)

        post_body = {
            "kind": "blogger#post",
            "title": article["title"],
            "content": content_html,
        }

        post = service.posts().insert(
            blogId=BLOG_ID,
            body=post_body,
            isDraft=False
        ).execute()

        print(f'âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø®Ø¨Ø±: {post["title"]}')

        # Ù†Ø­ÙØ¸Ù‡ ÙÙŠ Ø§Ù„Ù…Ù„Ù
        posted_titles.add(article["title"])
        with open("posted_titles.txt", "a", encoding="utf-8") as f:
            f.write(article["title"] + "\n")

    except HttpError as e:
        # Ù‡Ù†Ø§ Ù„Ùˆ Ø·Ù„Ø¹ 403 Ø®Ù„Ø§Øµ Ù†Ø¹Ø±Ù Ø¥Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ø§ ÙŠØ³Ù…Ø­ Ø¨Ø§Ù„Ù†Ø´Ø±
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Blogger: {e}")

# =========================
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# =========================
def main():
    print("ğŸŸ£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª...")
    posted_titles = load_posted_titles()
    articles = fetch_articles()

    if not articles:
        print("âŒ Ù…Ø§ Ù‚Ø¯Ø±Ù†Ø§ Ù†Ø¬ÙŠØ¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø£ÙŠ Ù…ØµØ¯Ø±. Ø¬Ø±Ø¨ Ø±Ø§Ø¨Ø· RSS Ù…Ø®ØªÙ„Ù.")
        return

    # Ù†Ø´ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    unique_articles = []
    seen_titles = set()
    for art in articles:
        if art["title"] not in seen_titles:
            unique_articles.append(art)
            seen_titles.add(art["title"])

    # Ù†Ù†Ø´Ø± Ø£ÙˆÙ„ 5 Ø¨Ø³
    for article in unique_articles[:5]:
        post_to_blogger(article, posted_titles)

    print("âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø³ÙƒØ±Ø¨Øª.")

if __name__ == "__main__":
    main()
