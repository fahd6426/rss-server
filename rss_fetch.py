import os
import requests
from bs4 import BeautifulSoup
import random

# ูุงุฎุฐ ุงูุฑุงุจุท ูุงูุณุฑ ูู ุณููุฑุชุณ ุฌithub
WEBHOOK_URL = os.getenv("WEBHOOK_URL")
WEBHOOK_SECRET = os.getenv("WEBHOOK_SECRET")

# ูุตุงุฏุฑ RSS ูุน ุงุณู ุงููุตุฏุฑ
RSS_FEEDS = [
    ("https://feeds.bbci.co.uk/sport/rss.xml", "BBC Sport"),
    ("https://feeds.skynews.com/feeds/rss/sports.xml", "Sky News - Sports"),
]

def translate_text(text, target_lang="ar"):
    """ุชุฑุฌูุฉ ุจุณูุทุฉ ุนุจุฑ ุฎุฏูุฉ ุฌูุฌู ุงููุฌุงููุฉ."""
    if not text:
        return ""
    url = "https://translate.googleapis.com/translate_a/single"
    params = {
        "client": "gtx",
        "sl": "auto",
        "tl": target_lang,
        "dt": "t",
        "q": text
    }
    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        data = r.json()
        return "".join([part[0] for part in data[0]])
    except Exception:
        # ูู ูุดูุช ุงูุชุฑุฌูุฉ ููุดุฑ ุงููุต ุงูุฃุตูู
        return text

def build_article(ar_title: str, ar_content: str, source_name: str) -> str:
    """ูุทููู ุงูุฎุจุฑ ููุถูู ููุงุท ููุตุฏุฑ."""
    intros = [
        "ููุฏูู ููู ููุฎุต ุงูุฎุจุฑ ุงูุฑูุงุถู ุงูุชุงูู:",
        "ุถูู ูุชุงุจุนุชูุง ุงูููููุฉ ูุฃูู ุงูุฃุฎุจุงุฑ ุงูุฑูุงุถูุฉ:",
        "ุฅูููู ุชูุงุตูู ุงูุฎุจุฑ ููุง ูุฑุฏ:"
    ]
    outros = [
        "ุชุงุจุนููุง ููุฒูุฏ ูู ุงูุฃุฎุจุงุฑ ุงูุฑูุงุถูุฉ ุฃููุงู ุจุฃูู.",
        "ููุงูููู ุจูู ุฌุฏูุฏ ุญุงู ุตุฏูุฑู.",
        "ุฒูุฑูุง ุงููุฏููุฉ ุจุงุณุชูุฑุงุฑ ูููุฒูุฏ."
    ]

    intro = random.choice(intros)
    outro = random.choice(outros)

    # ูู ุงููุญุชูู ูุตูุฑ ููุฑูุฑู ุนุดุงู ูุตูุฑ ุฃุทูู
    body = ar_content.strip()
    if len(body) < 150:
        body = body + "\n\n" + ar_content.strip()

    bullets = [
        "ุฃูู ุงูููุงุท ูู ุงูุฎุจุฑ:",
        f"- ุงูุนููุงู: {ar_title}",
        "- ุงูุฎุจุฑ ูู ูุตุฏุฑ ููุซูู.",
        "- ุงูุชูุงุตูู ุงููุงููุฉ ูุฐููุฑุฉ ุฃุนูุงู."
    ]
    bullets_text = "\n".join(bullets)

    source_line = f"\nุงููุตุฏุฑ: {source_name}"

    article = f"""{intro}

{body}

{bullets_text}

{outro}
{source_line}
"""
    return article

def fetch_articles():
    all_articles = []
    print("๐ ุจุฏุก ุฌูุจ ุงูุฃุฎุจุงุฑ ูู ุงููุตุงุฏุฑ ...")
    for feed_url, source_name in RSS_FEEDS:
        print(f"๐ก ุฌูุจ ูู: {feed_url}")
        try:
            resp = requests.get(feed_url, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            print(f"โ ูุดู ุฌูุจ ุงููุตุฏุฑ {feed_url}: {e}")
            continue

        soup = BeautifulSoup(resp.content, "lxml-xml")
        items = soup.find_all("item")

        for item in items:
            title = item.title.get_text(strip=True) if item.title else ""
            description = item.description.get_text(strip=True) if item.description else ""

            # ุญุงูู ูููุท ุตูุฑุฉ
            image_url = ""
            enclosure = item.find("enclosure")
            if enclosure and enclosure.get("url"):
                image_url = enclosure.get("url")

            if title:
                all_articles.append({
                    "title": title,
                    "content": description,
                    "image": image_url,
                    "source": source_name
                })
    print(f"๐ฆ ุฅุฌูุงูู ุงูุฃุฎุจุงุฑ: {len(all_articles)}")
    return all_articles
